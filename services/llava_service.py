
"""LLaVa ëª¨ë¸ ì„œë¹„ìŠ¤"""
########################################################
# LLaVa ëª¨ë¸ ë¡œë“œ ë° ì¶”ë¡  ì„œë¹„ìŠ¤
# 
# ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸:
# 1. llava-hf/llava-1.5-7b-hf (7B íŒŒë¼ë¯¸í„°, ê¶Œì¥)
# 2. llava-hf/llava-1.5-13b-hf (13B íŒŒë¼ë¯¸í„°, ë” ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
# 3. llava-hf/llava-1.5-7b-hf-merged (ë³‘í•©ëœ ë²„ì „)
#
# KoLLaVA ëª¨ë¸ ì‚¬ìš©ì€ í…ŒìŠ¤íŠ¸ í–ˆì„ ë•Œ ì˜ì–´ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” ê²ƒì„ í™•ì¸í•¨.
########################################################
# created_at: 2025-11-20
# updated_at: 2025-11-20
# author: LEEYH205
# description: LLaVa model service
# version: 0.1.0
# status: development
# tags: llava, model, service
# dependencies: transformers, torch, accelerate, pillow
# license: MIT
# copyright: 2025 FeedlyAI
########################################################

import os
import re
from typing import Optional, Dict, Any
from PIL import Image
import torch
from transformers import LlavaProcessor, LlavaForConditionalGeneration
from config import LLAVA_MODEL_NAME, DEVICE_TYPE, MODEL_DIR, USE_QUANTIZATION

# ë””ë°”ì´ìŠ¤ ì„¤ì •
DEVICE = DEVICE_TYPE if DEVICE_TYPE == "cuda" and torch.cuda.is_available() else "cpu"

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ (lazy loading)
_processor: Optional[LlavaProcessor] = None
_model: Optional[LlavaForConditionalGeneration] = None


def get_llava_model():
    """LLaVa ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´)"""
    global _processor, _model
    
    if _model is None or _processor is None:
        print(f"Loading LLaVa model: {LLAVA_MODEL_NAME} on {DEVICE}")
        print(f"Model will be saved to: {MODEL_DIR}")
        
        # Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬ë¥¼ model í´ë”ë¡œ ì„¤ì •
        # transformersëŠ” cache_dir ë‚´ì— models--{org}--{model-name} êµ¬ì¡°ë¡œ ì €ì¥
        os.environ["HF_HOME"] = MODEL_DIR
        os.environ["TRANSFORMERS_CACHE"] = MODEL_DIR
        
        # í”„ë¡œì„¸ì„œ ë¡œë“œ (ìë™ìœ¼ë¡œ MODEL_DIRì— ìºì‹œë¨)
        print(f"Downloading/loading processor from Hugging Face...")
        _processor = LlavaProcessor.from_pretrained(
            LLAVA_MODEL_NAME,
            cache_dir=MODEL_DIR
        )
        
        # ëª¨ë¸ ë¡œë“œ (ìë™ìœ¼ë¡œ MODEL_DIRì— ìºì‹œë¨)
        print(f"Downloading/loading model from Hugging Face...")
        print(f"Quantization setting: {'Enabled (8-bit)' if USE_QUANTIZATION else 'Disabled (FP16/FP32)'}")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ë¡œë“œ ì „)
        if DEVICE == "cuda":
            torch.cuda.reset_peak_memory_stats()
            initial_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        
        # ë©”ëª¨ë¦¬ ìµœì í™”: 8-bit ì–‘ìí™” ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¼ ì„ íƒ
        if DEVICE == "cuda" and USE_QUANTIZATION:
            try:
                from transformers import BitsAndBytesConfig
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    bnb_8bit_compute_dtype=torch.float16
                )
                _model = LlavaForConditionalGeneration.from_pretrained(
                    LLAVA_MODEL_NAME,
                    quantization_config=quantization_config,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    cache_dir=MODEL_DIR
                )
                print("âœ“ Model loaded with 8-bit quantization for memory efficiency")
            except Exception as e:
                print(f"âš  8-bit quantization failed: {e}")
                print("Falling back to standard loading with memory limits...")
                # 8-bit ì–‘ìí™” ì‹¤íŒ¨ ì‹œ ë©”ëª¨ë¦¬ ì œí•œê³¼ í•¨ê»˜ ë¡œë“œ
                _model = LlavaForConditionalGeneration.from_pretrained(
                    LLAVA_MODEL_NAME,
                    torch_dtype=torch.float16,
                    device_map="auto",
                    low_cpu_mem_usage=True,
                    cache_dir=MODEL_DIR,
                    max_memory={0: "20GiB"}  # GPU ë©”ëª¨ë¦¬ ì œí•œ
                )
                print("âœ“ Model loaded with FP16 (quantization disabled)")
        elif DEVICE == "cuda":
            # ì–‘ìí™” ë¹„í™œì„±í™”: FP16ìœ¼ë¡œ ë¡œë“œ
            _model = LlavaForConditionalGeneration.from_pretrained(
                LLAVA_MODEL_NAME,
                torch_dtype=torch.float16,
                device_map="auto",
                low_cpu_mem_usage=True,
                cache_dir=MODEL_DIR
            )
            print("âœ“ Model loaded with FP16 (quantization disabled)")
        else:
            # CPU ëª¨ë“œ
            _model = LlavaForConditionalGeneration.from_pretrained(
                LLAVA_MODEL_NAME,
                torch_dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=True,
                cache_dir=MODEL_DIR
            )
            _model = _model.to(DEVICE)
            print("âœ“ Model loaded on CPU")
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • (ë¡œë“œ í›„)
        if DEVICE == "cuda":
            loaded_memory = torch.cuda.memory_allocated() / 1024**3  # GB
            peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
            print(f"ğŸ“Š GPU Memory Usage:")
            print(f"   - Allocated: {loaded_memory:.2f} GB")
            print(f"   - Peak (during load): {peak_memory:.2f} GB")
            print(f"   - Total GPU: {total_memory:.2f} GB")
            print(f"   - Usage: {loaded_memory/total_memory*100:.1f}%")
        
        _model.eval()
        print(f"âœ“ LLaVa model loaded successfully on {DEVICE}")
        print(f"âœ“ Model cached in: {MODEL_DIR}")
    
    return _processor, _model


def process_image_with_llava(
    image: Image.Image,
    prompt: str,
    max_new_tokens: int = 512,

    # temperature ì¡°ì ˆ
    temperature: float = 0.1,
    # ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€
    do_sample: bool = False
) -> str:
    """
    LLaVaë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ë¥¼ ì²˜ë¦¬í•˜ê³  ì‘ë‹µ ìƒì„±
    
    Args:
        image: PIL Image ê°ì²´
        prompt: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
        temperature: ìƒì„± ì˜¨ë„
        do_sample: ìƒ˜í”Œë§ ì‚¬ìš© ì—¬ë¶€
    
    Returns:
        ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‘ë‹µ
    """
    processor, model = get_llava_model()
    
    # LLaVa-1.5 í”„ë¡¬í”„íŠ¸ í˜•ì‹: USER: <image>\n{prompt}\nASSISTANT:
    # ì´ë¯¸ì§€ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬í•˜ê³  í”„ë¡¬í”„íŠ¸ë¥¼ ì˜¬ë°”ë¥¸ í˜•ì‹ìœ¼ë¡œ êµ¬ì„±
    formatted_prompt = f"USER: <image>\n{prompt}\nASSISTANT:"
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if DEVICE == "cuda":
        torch.cuda.empty_cache()
    
    # ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ì´ë¯¸ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ë¡œ ì „ë‹¬)
    # ë©”ëª¨ë¦¬ ìµœì í™”: CPUì—ì„œ ì²˜ë¦¬ í›„ í•„ìš”ì‹œ GPUë¡œ ì´ë™
    inputs = processor(images=[image], text=formatted_prompt, return_tensors="pt")
    
    # GPUë¡œ ì´ë™ (8-bit ì–‘ìí™”ëœ ëª¨ë¸ì€ ìë™ìœ¼ë¡œ ì²˜ë¦¬ë¨)
    if DEVICE == "cuda":
        inputs = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
    
    # ì¶”ë¡ 
    with torch.no_grad():
        generate_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=do_sample,
            pad_token_id=processor.tokenizer.eos_token_id if processor.tokenizer.pad_token_id is None else processor.tokenizer.pad_token_id
        )
    
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if DEVICE == "cuda":
        del inputs
        torch.cuda.empty_cache()
    
    # ì‘ë‹µ ë””ì½”ë”©
    response = processor.batch_decode(
        generate_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]
    
    # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±° (ì‘ë‹µë§Œ ë°˜í™˜)
    # ASSISTANT: ì´í›„ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    if "ASSISTANT:" in response:
        response = response.split("ASSISTANT:")[-1].strip()
    elif formatted_prompt in response:
        response = response.replace(formatted_prompt, "").strip()
    
    return response


def validate_image_and_text(
    image: Image.Image,
    ad_copy_text: Optional[str] = None,
    validation_prompt: Optional[str] = None
) -> Dict[str, Any]:
    """
    Stage 1: ì´ë¯¸ì§€ì™€ ê´‘ê³ ë¬¸êµ¬ì˜ ì í•©ì„± ê²€ì¦
    
    Args:
        image: PIL Image ê°ì²´
        ad_copy_text: ê´‘ê³  ë¬¸êµ¬ í…ìŠ¤íŠ¸
        validation_prompt: ê²€ì¦ìš© í”„ë¡¬í”„íŠ¸
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    # Step 1: ì´ë¯¸ì§€ë§Œ ë¨¼ì € ë¶„ì„ (ê´‘ê³  ë¬¸êµ¬ì˜ ì˜í–¥ì„ ë°›ì§€ ì•Šë„ë¡)
    image_analysis_prompt = """Analyze this image ONLY. Do NOT consider any ad copy text. Focus solely on what you see in the image.

## Image Analysis (IMPORTANT: Analyze ONLY the image, ignore any text that might be mentioned)
- Product shown: [What food/product is actually visible? Be specific with the exact name]
- Characteristics: [spicy/mild, color, ingredients visible, texture, etc.]
- Setting: [home/restaurant/office/etc.]
- Mood: [cozy/formal/casual/etc.]

Focus on the product characteristics and visual elements."""
    
    # ì´ë¯¸ì§€ë§Œ ë¨¼ì € ë¶„ì„
    image_analysis = process_image_with_llava(image, image_analysis_prompt)
    
    if validation_prompt is None:
        # Step 2: ê´‘ê³  ë¬¸êµ¬ì™€ ë¹„êµ
        if ad_copy_text:
            validation_prompt = f"""You are evaluating whether an ad copy text matches an advertisement image. 

## 1. Image Analysis (Already completed - DO NOT re-analyze)
{image_analysis}

## 2. Ad Copy Analysis
Ad copy: "{ad_copy_text}"
- Product mentioned: [exact name from ad]
- Characteristics: [spicy/mild/etc. from ad]
- Target audience: [ONLY extract if explicitly mentioned. Look for "for people who...", "for [group]", "target audience". If NOT mentioned, write "none". Examples: "for people who hate spicy" = "people who hate spicy", "for spicy lovers" = "spicy lovers", no mention = "none"]
- Message: [main point]

## 3. Compatibility Check
STEP 1: Logical consistency (CRITICAL)
Check if target audience conflicts with product characteristics:
- If target audience = "people who hate spicy" AND product contains "spicy" â†’ CONTRADICTION â†’ Logical Consistency = No
- If target audience = "spicy lovers" AND product is "spicy" â†’ NO CONTRADICTION â†’ Logical Consistency = Yes
- If target audience = "none" (not mentioned) â†’ NO CONTRADICTION â†’ Logical Consistency = Yes

CRITICAL RULES:
- "hate spicy" + "spicy [product]" = CONTRADICTION â†’ Logical Consistency = No
- "dislike spicy" + "spicy [product]" = CONTRADICTION â†’ Logical Consistency = No
- "spicy [product]" + no target audience = NO CONTRADICTION â†’ Logical Consistency = Yes
- "spicy [product]" + "for spicy lovers" = NO CONTRADICTION â†’ Logical Consistency = Yes

## 4. Final Assessment (EXACT format)
Match Score: [0-10]/10
Logical Consistency: [Yes/No - No if "hate spicy" + "spicy product"]
Mismatch Detected: [Yes/No]
Mismatch Details: [List issues or "None"]
Overall Assessment: [Suitable/Not Suitable]
Reasoning: [Brief explanation]

CRITICAL SCORING RULES (MUST FOLLOW):
- If Logical Consistency = No (target audience conflicts with product characteristics) â†’ Match Score MUST be 0-2/10, Mismatch Detected = Yes, Not Suitable
- If Mismatch Detected = Yes â†’ Match Score MUST be 0-2/10, Not Suitable
- If Logical Consistency = Yes AND Mismatch Detected = No â†’ Match Score can be 7-10/10, Suitable

RULES:
- If Logical Consistency = No â†’ Mismatch Detected = Yes, Overall Assessment = Not Suitable, Match Score = 0-2/10
- If target audience explicitly states they dislike a characteristic that the product has â†’ Logical Consistency = No, Mismatch Detected = Yes, Not Suitable, Match Score = 0-2/10
- If target audience = "none" â†’ Logical Consistency = Yes (no contradiction to check)
- Any contradiction or mismatch â†’ Not Suitable, Match Score = 0-2/10"""
        else:
            validation_prompt = image_analysis_prompt + "\n\n3. Provide general recommendations for advertising use.\n\nProvide your analysis."
    
    # Step 2: ê´‘ê³  ë¬¸êµ¬ì™€ ë¹„êµ (ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ í¬í•¨)
    response = process_image_with_llava(image, validation_prompt)
    
    # ì‘ë‹µ íŒŒì‹± - ê°œì„ ëœ ë¡œì§
    response_lower = response.lower()
    
    # ë¶ˆì¼ì¹˜ ê°ì§€ (êµ¬ì¡°í™”ëœ í˜•ì‹ ìš°ì„ )
    has_mismatch = False
    mismatch_details = ""
    logical_consistency = None
    
    # í›„ì²˜ë¦¬: ê´‘ê³  ë¬¸êµ¬ì—ì„œ ì§ì ‘ ë…¼ë¦¬ì  ëª¨ìˆœ íŒ¨í„´ ì°¾ê¸° (LLaVa ì‘ë‹µë³´ë‹¤ ìš°ì„ )
    direct_logical_contradiction = False
    if ad_copy_text:
        ad_lower = ad_copy_text.lower()
        # "hate spicy", "dislike spicy", "don't like spicy" ë“±ì˜ íŒ¨í„´ ì°¾ê¸°
        negative_patterns = [
            r'hate\s+spicy',
            r'dislike\s+spicy',
            r"don'?t\s+like\s+spicy",
            r'not\s+for\s+.*spicy',
            r'avoid\s+spicy',
            r'people\s+who\s+hate\s+spicy',
            r'people\s+who\s+dislike\s+spicy',
        ]
        # ê´‘ê³  ë¬¸êµ¬ì— "spicy"ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        has_spicy_in_ad = 'spicy' in ad_lower
        
        # íƒ€ê²Ÿ ì˜¤ë””ì–¸ìŠ¤ê°€ ë§¤ìš´ ê²ƒì„ ì‹«ì–´í•˜ëŠ”ë° ì œí’ˆì´ ë§¤ìš´ ê²½ìš°
        for pattern in negative_patterns:
            if re.search(pattern, ad_lower, re.IGNORECASE):
                if has_spicy_in_ad:
                    direct_logical_contradiction = True
                    logical_consistency = False
                    has_mismatch = True
                    mismatch_details = "Target audience dislikes spicy food but product is described as spicy"
                    break
    
    # Logical Consistency í™•ì¸ (LLaVa ì‘ë‹µì—ì„œ, í›„ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ì„ ë•Œë§Œ)
    if logical_consistency is None:
        logical_consistency_match = re.search(r'logical\s+consistency[:\s]+(yes|no)', response_lower)
        if logical_consistency_match:
            logical_consistency = logical_consistency_match.group(1).lower() == "yes"
            if not logical_consistency:
                has_mismatch = True
    
    # êµ¬ì¡°í™”ëœ í˜•ì‹ì—ì„œ ë¶ˆì¼ì¹˜ í™•ì¸
    mismatch_detected_match = re.search(r'mismatch\s+detected[:\s]+(yes|no)', response_lower)
    if mismatch_detected_match:
        mismatch_detected = mismatch_detected_match.group(1).lower() == "yes"
        if mismatch_detected:
            has_mismatch = True
        # ë¶ˆì¼ì¹˜ ìƒì„¸ ì •ë³´ ì¶”ì¶œ
        mismatch_details_match = re.search(r'mismatch\s+details[:\s]+([^\n]+?)(?:\n|Overall|Reasoning)', response_lower, re.IGNORECASE | re.DOTALL)
        if mismatch_details_match:
            mismatch_details = mismatch_details_match.group(1).strip()
            if mismatch_details.lower() != "none" and len(mismatch_details) > 5:
                has_mismatch = True
    else:
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶ˆì¼ì¹˜ ê°ì§€ (fallback)
        mismatch_keywords = [
            "mismatch", "doesn't match", "does not match", "contradict", 
            "inappropriate", "incorrect", "wrong context", "different setting",
            "not match", "unmatch", "conflict", "discrepancy", "different product"
        ]
        has_mismatch = any(keyword in response_lower for keyword in mismatch_keywords)
    
    # ì ìˆ˜ ì¶”ì¶œ (êµ¬ì¡°í™”ëœ í˜•ì‹ ìš°ì„ )
    relevance_score = None
    
    # êµ¬ì¡°í™”ëœ í˜•ì‹ì—ì„œ ì ìˆ˜ ì¶”ì¶œ (ìš°ì„ ìˆœìœ„ 1)
    # ë²”ìœ„ í˜•ì‹ ì²˜ë¦¬ (ì˜ˆ: "7-10/10" -> ìµœì†Œê°’ ì‚¬ìš©)
    structured_score_match = re.search(r'match\s+score[:\s]+(\d+(?:\.\d+)?)(?:\s*-\s*\d+)?\s*/10', response_lower)
    if structured_score_match:
        score_value = float(structured_score_match.group(1))
        relevance_score = score_value / 10.0
    else:
        # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ì ìˆ˜ ì°¾ê¸° (ìš°ì„ ìˆœìœ„ 2)
        score_patterns = [
            r'rating[:\s]+(\d+(?:\.\d+)?)(?:\s*-\s*\d+)?\s*/10',  # "Rating: 10/10" or "Rating: 7-10/10"
            r'score[:\s]+(\d+(?:\.\d+)?)(?:\s*-\s*\d+)?\s*/10',   # "Score: 9/10" or "Score: 7-10/10"
            r'match[:\s]+(\d+(?:\.\d+)?)(?:\s*-\s*\d+)?\s*/10',   # "Match: 8/10" or "Match: 7-10/10"
            r'(\d+(?:\.\d+)?)(?:\s*-\s*\d+)?\s*/10',              # "10/10" or "7-10/10"
            r'(\d+(?:\.\d+)?)\s+on\s+the\s+scale', # "9 on the scale"
            r'rate[:\s]+(\d+(?:\.\d+)?)',          # "Rate: 8"
        ]
        for pattern in score_patterns:
            score_match = re.search(pattern, response_lower)
            if score_match:
                score_value = float(score_match.group(1))
                # 10ì  ë§Œì ì¸ ê²½ìš°ë§Œ ì •ê·œí™”
                if '/10' in pattern or 'scale' in pattern:
                    relevance_score = score_value / 10.0
                else:
                    # ì´ë¯¸ 0-1 ìŠ¤ì¼€ì¼ì¸ ê²½ìš°
                    relevance_score = min(score_value, 1.0)
                break
    
    if relevance_score is None:
        # ì ìˆ˜ê°€ ì—†ìœ¼ë©´ ë¶ˆì¼ì¹˜ ì—¬ë¶€ë¡œ íŒë‹¨
        if has_mismatch:
            relevance_score = 0.3  # ë¶ˆì¼ì¹˜ ê°ì§€ ì‹œ ë‚®ì€ ì ìˆ˜
        elif "perfect match" in response_lower or "excellent match" in response_lower:
            relevance_score = 0.95
        elif "good match" in response_lower or "matches well" in response_lower:
            relevance_score = 0.8
        elif "suitable" in response_lower or "match" in response_lower:
            relevance_score = 0.6
        else:
            relevance_score = 0.5
    
    # ì ìˆ˜ ì¡°ì •: ëª…í™•í•œ ë¶ˆì¼ì¹˜ê°€ ìˆìœ¼ë©´ ê°•ì œë¡œ ë‚®ì€ ì ìˆ˜ ë¶€ì—¬
    # LLaVaê°€ ë†’ì€ ì ìˆ˜ë¥¼ ì¤˜ë„ ë¶ˆì¼ì¹˜ê°€ ìˆìœ¼ë©´ ë‚®ì¶¤
    # ë¶ˆì¼ì¹˜ê°€ ìˆìœ¼ë©´ ì ìˆ˜ ì°¨ì´ë¥¼ ë” ëª…í™•í•˜ê²Œ (0.1-0.2 ë²”ìœ„)
    max_score_for_mismatch = 0.2
    min_score_for_mismatch = 0.1
    
    # ë…¼ë¦¬ì  ëª¨ìˆœì´ ê°ì§€ë˜ë©´ ë¬´ì¡°ê±´ ë‚®ì€ ì ìˆ˜ (í›„ì²˜ë¦¬ ê²°ê³¼ ìš°ì„ )
    if logical_consistency is False:
        # ë…¼ë¦¬ì  ëª¨ìˆœ â†’ 0.1ë¡œ ê°•ì œ ì¡°ì • (ë” ë‚®ì€ ì ìˆ˜)
        relevance_score = min_score_for_mismatch
    elif has_mismatch:
        # ê¸°íƒ€ ë¶ˆì¼ì¹˜ ê°ì§€ â†’ 0.1-0.2 ë²”ìœ„ë¡œ ê°•ì œ ì¡°ì •
        relevance_score = min(relevance_score, max_score_for_mismatch) if relevance_score else min_score_for_mismatch
    
    # ì í•©ì„± íŒë‹¨ (êµ¬ì¡°í™”ëœ í˜•ì‹ ìš°ì„ )
    is_valid = None
    overall_assessment_match = re.search(r'overall\s+assessment[:\s]+(suitable|not\s+suitable)', response_lower)
    if overall_assessment_match:
        is_valid = overall_assessment_match.group(1).lower().replace(" ", "") == "suitable"
        # Overall Assessmentê°€ Suitableì—¬ë„ ë¶ˆì¼ì¹˜ê°€ ìˆìœ¼ë©´ Falseë¡œ ë³€ê²½
        if is_valid and (logical_consistency is False or has_mismatch):
            is_valid = False
    else:
        # Logical Consistencyê°€ Noì´ë©´ ìë™ìœ¼ë¡œ Not Suitable (ìµœìš°ì„ )
        if logical_consistency is False:
            is_valid = False
        # ì ìˆ˜ ê¸°ë°˜ íŒë‹¨ (fallback)
        elif relevance_score is not None:
            is_valid = relevance_score >= 0.7 and not has_mismatch
        else:
            # í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨
            is_valid = "suitable" in response_lower and not has_mismatch and (logical_consistency is not False)
    
    image_quality_ok = "quality" in response_lower and ("good" in response_lower or "high" in response_lower or "excellent" in response_lower)
    
    # ì´ìŠˆ ì¶”ì¶œ
    issues = []
    if has_mismatch:
        if mismatch_details and mismatch_details.lower() != "none":
            issues.append(mismatch_details)
        else:
            # ë¶ˆì¼ì¹˜ ë‚´ìš© ì¶”ì¶œ
            mismatch_section = re.search(r'mismatch[^.]*\.', response_lower, re.IGNORECASE)
            if mismatch_section:
                issues.append(mismatch_section.group(0))
            else:
                issues.append("Context mismatch detected between image and ad copy")
    
    return {
        "is_valid": is_valid,
        "image_quality_ok": image_quality_ok,
        "relevance_score": relevance_score,
        "analysis": response,
        "issues": issues,
        "recommendations": []
    }


def judge_final_ad(
    image: Image.Image,
    judge_prompt: Optional[str] = None
) -> Dict[str, Any]:
    """
    Stage 2: ìµœì¢… ê´‘ê³  ì‹œê° ê²°ê³¼ë¬¼ íŒë‹¨
    
    Args:
        image: PIL Image ê°ì²´ (ìµœì¢… ê´‘ê³  ì´ë¯¸ì§€)
        judge_prompt: íŒë‹¨ìš© í”„ë¡¬í”„íŠ¸
    
    Returns:
        íŒë‹¨ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    if judge_prompt is None:
        judge_prompt = """Analyze this final advertisement image and evaluate:
1. Does it follow the advertising brief? (on_brief)
2. Is there any text or important content occluded? (occlusion)
3. Is the contrast between text and background appropriate? (contrast_ok)
4. Is there a clear call-to-action (CTA) present? (cta_present)
5. List any issues or problems you find.

Provide your analysis in a structured format."""
    
    response = process_image_with_llava(image, judge_prompt)
    
    # ì‘ë‹µ íŒŒì‹± (ê°„ë‹¨í•œ ì˜ˆì œ, ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ íŒŒì‹± í•„ìš”)
    on_brief = "brief" in response.lower() and ("follow" in response.lower() or "yes" in response.lower())
    occlusion = "occlude" in response.lower() and "no" in response.lower()
    contrast_ok = "contrast" in response.lower() and ("good" in response.lower() or "appropriate" in response.lower())
    cta_present = "cta" in response.lower() or "call-to-action" in response.lower()
    
    issues = []
    if "issue" in response.lower() or "problem" in response.lower():
        # TODO: ì‹¤ì œ ì´ìŠˆ ì¶”ì¶œ ë¡œì§ êµ¬í˜„
        issues = ["Some issues detected - check analysis"]
    
    return {
        "on_brief": on_brief,
        "occlusion": not occlusion,  # occlusionì´ Falseë©´ ê°€ë¦¼ ì—†ìŒ
        "contrast_ok": contrast_ok,
        "cta_present": cta_present,
        "analysis": response,
        "issues": issues
    }


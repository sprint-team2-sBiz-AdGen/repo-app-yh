# LLaVA í†µí•© ë°œí‘œìë£Œ

## ğŸ“‹ ê°œìš”

**ê¸°ëŠ¥ëª…**: LLaVA (Large Language and Vision Assistant) í†µí•©

**ëª©ì **: ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì˜ ì¼ê´€ì„± ê²€ì¦ ë° í’ˆì§ˆ í‰ê°€ë¥¼ ìœ„í•œ ë©€í‹°ëª¨ë‹¬ AI ëª¨ë¸ í†µí•©

**í•µì‹¬ ê°€ì¹˜**: 
- ë©€í‹°ëª¨ë‹¬ AI í™œìš© (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸ ë™ì‹œ ì²˜ë¦¬)
- GPU íš¨ìœ¨ì  ì‚¬ìš© (8-bit ì–‘ìí™” ì§€ì›)
- Thread-safe ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- 2ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ (Stage 1: ì´ˆê¸° ê²€ì¦, Stage 2: ìµœì¢… í’ˆì§ˆ í‰ê°€)

---

## ğŸ¯ ëª©ì 

### Stage 1: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ì¦
- **ëª©ì **: ìƒì„±ëœ ì´ë¯¸ì§€ì™€ ê´‘ê³ ë¬¸êµ¬ì˜ ì í•©ì„± ê²€ì¦
- **í™œìš©**: ê´‘ê³  ì´ë¯¸ì§€ì™€ ê´‘ê³ ë¬¸êµ¬ì˜ ë…¼ë¦¬ì  ì¼ê´€ì„± í™•ì¸
- **ì¶œë ¥**: ê´€ë ¨ì„± ì ìˆ˜, ì´ìŠˆ ëª©ë¡, ì¶”ì²œì‚¬í•­, í°íŠ¸ ì¶”ì²œ

### Stage 2: ìµœì¢… í’ˆì§ˆ í‰ê°€
- **ëª©ì **: ì˜¤ë²„ë ˆì´ëœ ì´ë¯¸ì§€ì˜ ìµœì¢… í’ˆì§ˆ í‰ê°€
- **í™œìš©**: í…ìŠ¤íŠ¸ ê°€ë¦¼, ëŒ€ë¹„, CTA ì¡´ì¬ ì—¬ë¶€ ë“± í’ˆì§ˆ ìš”ì†Œ ê²€ì¦
- **ì¶œë ¥**: brief ì¤€ìˆ˜ ì—¬ë¶€, ê°€ë¦¼ ì—¬ë¶€, ëŒ€ë¹„ ì ì ˆì„±, CTA ì¡´ì¬ ì—¬ë¶€

---

## ğŸ”§ ì£¼ìš” íŠ¹ì§•

### 1. Thread-safe ëª¨ë¸ ë¡œë”©
- **ì‹±ê¸€í†¤ íŒ¨í„´**: ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì‚¬ìš©
- **Double-checked locking**: ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ ë™ì‹œì— ì ‘ê·¼í•´ë„ ì•ˆì „í•˜ê²Œ ë¡œë“œ
- **Lazy loading**: í•„ìš”í•  ë•Œë§Œ ëª¨ë¸ ë¡œë“œ
- **Race condition ë°©ì§€**: ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ëª¨ë¸ ì¤‘ë³µ ë¡œë”© ë°©ì§€

### 2. 8-bit ì–‘ìí™” ì§€ì›
- **ë©”ëª¨ë¦¬ ì ˆì•½**: FP16 ëŒ€ë¹„ ì•½ 50% ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- **ì„±ëŠ¥ ìœ ì§€**: ì¶”ë¡  ì •í™•ë„ ê±°ì˜ ìœ ì§€
- **ìë™ fallback**: ì–‘ìí™” ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ FP16/FP32ë¡œ ì „í™˜
- **BitsAndBytesConfig**: Hugging Faceì˜ ì–‘ìí™” ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©
- **ë™ì  ì–‘ìí™”**: ì¶”ë¡  ì‹œ ìë™ìœ¼ë¡œ ì–‘ìí™”/ì—­ì–‘ìí™” ìˆ˜í–‰

### 3. GPU ê¸°ë°˜ ì¶”ë¡ 
- **CUDA ì§€ì›**: GPU ê°€ì†ìœ¼ë¡œ ë¹ ë¥¸ ì¶”ë¡ 
- **ë©”ëª¨ë¦¬ ê´€ë¦¬**: ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ë° ìµœì í™”
- **ë””ë°”ì´ìŠ¤ ìë™ ì„ íƒ**: CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ìœ¼ë¡œ GPU ì‚¬ìš©

### 4. 2ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œ
- **Stage 1**: ì´ˆê¸° ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ì¦
- **Stage 2**: ìµœì¢… ì˜¤ë²„ë ˆì´ í’ˆì§ˆ í‰ê°€

---

## ğŸ“ êµ¬í˜„ ìœ„ì¹˜

### ì„œë¹„ìŠ¤ ë ˆì´ì–´
- `services/llava_service.py`: LLaVA ëª¨ë¸ ì„œë¹„ìŠ¤ (ëª¨ë¸ ë¡œë”©, ì¶”ë¡  ë¡œì§)

### API ì—”ë“œí¬ì¸íŠ¸
- `routers/llava_stage1.py`: Stage 1 API ì—”ë“œí¬ì¸íŠ¸ (`/api/yh/llava/stage1/validate`)
- `routers/llava_stage2.py`: Stage 2 API ì—”ë“œí¬ì¸íŠ¸ (`/api/yh/llava/stage2/judge`)

### ë°ì´í„°ë² ì´ìŠ¤
- `vlm_traces` í…Œì´ë¸”: ëª¨ë“  LLaVA í˜¸ì¶œ ì¶”ì  ë° ì €ì¥

---

## ğŸ’» êµ¬í˜„ ì½”ë“œ

### 1. Thread-safe ëª¨ë¸ ë¡œë”©

**íŒŒì¼**: `services/llava_service.py`

```python
import threading
from transformers import LlavaProcessor, LlavaForConditionalGeneration
from config import LLAVA_MODEL_NAME, DEVICE_TYPE, MODEL_DIR, USE_QUANTIZATION

# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ (lazy loading)
_processor: Optional[LlavaProcessor] = None
_model: Optional[LlavaForConditionalGeneration] = None
_model_lock = threading.Lock()  # ëª¨ë¸ ë¡œë”© ë™ê¸°í™”ë¥¼ ìœ„í•œ ë½

def get_llava_model():
    """LLaVA ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ (ì‹±ê¸€í†¤ íŒ¨í„´, thread-safe)"""
    global _processor, _model
    
    # Double-checked locking íŒ¨í„´ìœ¼ë¡œ thread-safeí•˜ê²Œ ëª¨ë¸ ë¡œë”©
    if _model is None or _processor is None:
        with _model_lock:
            # ë‹¤ì‹œ í™•ì¸ (ë‹¤ë¥¸ ìŠ¤ë ˆë“œê°€ ì´ë¯¸ ë¡œë”©í–ˆì„ ìˆ˜ ìˆìŒ)
            if _model is None or _processor is None:
                print(f"Loading LLaVa model: {LLAVA_MODEL_NAME} on {DEVICE}")
                print(f"Model will be saved to: {MODEL_DIR}")
                
                # Hugging Face ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
                os.environ["HF_HOME"] = MODEL_DIR
                os.environ["TRANSFORMERS_CACHE"] = MODEL_DIR
                
                # í”„ë¡œì„¸ì„œ ë¡œë“œ
                _processor = LlavaProcessor.from_pretrained(
                    LLAVA_MODEL_NAME,
                    cache_dir=MODEL_DIR
                )
                
                # ëª¨ë¸ ë¡œë“œ (8-bit ì–‘ìí™” ì§€ì›)
                if DEVICE == "cuda" and USE_QUANTIZATION:
                    from transformers import BitsAndBytesConfig
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                        bnb_8bit_compute_dtype=torch.float16
                    )
                    _model = LlavaForConditionalGeneration.from_pretrained(
                        LLAVA_MODEL_NAME,
                        quantization_config=quantization_config,
                        device_map="auto",
                        low_cpu_mem_usage=True,
                        cache_dir=MODEL_DIR
                    )
                    print("âœ“ Model loaded with 8-bit quantization")
                else:
                    _model = LlavaForConditionalGeneration.from_pretrained(
                        LLAVA_MODEL_NAME,
                        cache_dir=MODEL_DIR,
                        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
                        device_map="auto"
                    )
                
                logger.info(f"âœ“ LLaVA ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {LLAVA_MODEL_NAME}")
    
    return _processor, _model
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- **Double-checked locking**: ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ ë™ì‹œì— ì ‘ê·¼í•´ë„ ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
- **8-bit ì–‘ìí™”**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•½ 50% ê°ì†Œ
- **Lazy loading**: í•„ìš”í•  ë•Œë§Œ ë¡œë“œ
- **ìë™ ë””ë°”ì´ìŠ¤ ì„ íƒ**: CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ ìë™ìœ¼ë¡œ GPU ì‚¬ìš©

---

### 1-1. Thread-safe ëª¨ë¸ ë¡œë”© ìƒì„¸ ì„¤ëª…

#### ë¬¸ì œ ìƒí™©: Race Condition

ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì—ì„œ ì—¬ëŸ¬ ìš”ì²­ì´ ë™ì‹œì— ë“¤ì–´ì˜¬ ë•Œ, ê° ìŠ¤ë ˆë“œê°€ ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ê³  ì‹œë„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
Thread 1: _modelì´ Noneì¸ì§€ í™•ì¸ â†’ True
Thread 2: _modelì´ Noneì¸ì§€ í™•ì¸ â†’ True (Thread 1ì´ ì•„ì§ ë¡œë”© ì¤‘)
Thread 1: ëª¨ë¸ ë¡œë”© ì‹œì‘...
Thread 2: ëª¨ë¸ ë¡œë”© ì‹œì‘... (ì¤‘ë³µ ë¡œë”©!)
```

**ê²°ê³¼**: 
- ëª¨ë¸ì´ ì—¬ëŸ¬ ë²ˆ ë¡œë“œë˜ì–´ ë©”ëª¨ë¦¬ ë‚­ë¹„
- GPU ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥
- ë¡œë”© ì‹œê°„ ì¦ê°€

#### í•´ê²° ë°©ë²•: Double-Checked Locking íŒ¨í„´

```python
# ì „ì—­ ëª¨ë¸ ë³€ìˆ˜ (lazy loading)
_processor: Optional[LlavaProcessor] = None
_model: Optional[LlavaForConditionalGeneration] = None
_model_lock = threading.Lock()  # ëª¨ë¸ ë¡œë”© ë™ê¸°í™”ë¥¼ ìœ„í•œ ë½

def get_llava_model():
    global _processor, _model
    
    # ì²« ë²ˆì§¸ ì²´í¬: ë½ ì—†ì´ ë¹ ë¥´ê²Œ í™•ì¸ (ì„±ëŠ¥ ìµœì í™”)
    if _model is None or _processor is None:
        # ë‘ ë²ˆì§¸ ì²´í¬: ë½ì„ íšë“í•œ í›„ ë‹¤ì‹œ í™•ì¸ (race condition ë°©ì§€)
        with _model_lock:
            if _model is None or _processor is None:
                # ì‹¤ì œ ëª¨ë¸ ë¡œë”© (í•œ ìŠ¤ë ˆë“œë§Œ ì‹¤í–‰)
                _processor = LlavaProcessor.from_pretrained(...)
                _model = LlavaForConditionalGeneration.from_pretrained(...)
    
    return _processor, _model
```

**ë™ì‘ ì›ë¦¬**:

1. **ì²« ë²ˆì§¸ ì²´í¬ (ë½ ì—†ì´)**:
   - ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ëª¨ë¸ì´ ì´ë¯¸ ë¡œë“œë˜ì–´ ìˆìœ¼ë¯€ë¡œ ë¹ ë¥´ê²Œ ë°˜í™˜
   - ë½ì„ íšë“í•˜ì§€ ì•Šì•„ ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”

2. **ë‘ ë²ˆì§¸ ì²´í¬ (ë½ íšë“ í›„)**:
   - ì—¬ëŸ¬ ìŠ¤ë ˆë“œê°€ ë™ì‹œì— ì²« ë²ˆì§¸ ì²´í¬ë¥¼ í†µê³¼í–ˆì„ ë•Œ
   - ë½ì„ íšë“í•œ ìŠ¤ë ˆë“œë§Œ ëª¨ë¸ ë¡œë”© ì‹¤í–‰
   - ë‹¤ë¥¸ ìŠ¤ë ˆë“œë“¤ì€ ë½ í•´ì œ í›„ ì´ë¯¸ ë¡œë“œëœ ëª¨ë¸ ì‚¬ìš©

**íƒ€ì„ë¼ì¸ ì˜ˆì‹œ**:

```
ì‹œê°„ | Thread 1                    | Thread 2                    | Thread 3
-----|----------------------------|----------------------------|----------------------------
T1   | ì²« ë²ˆì§¸ ì²´í¬: None          | ì²« ë²ˆì§¸ ì²´í¬: None          | ì²« ë²ˆì§¸ ì²´í¬: None
T2   | ë½ íšë“ ì‹œë„                | ë½ íšë“ ëŒ€ê¸°                | ë½ íšë“ ëŒ€ê¸°
T3   | ë½ íšë“ ì„±ê³µ                | ë½ íšë“ ëŒ€ê¸°                | ë½ íšë“ ëŒ€ê¸°
T4   | ë‘ ë²ˆì§¸ ì²´í¬: None          |                             |
T5   | ëª¨ë¸ ë¡œë”© ì‹œì‘              |                             |
T6   | ëª¨ë¸ ë¡œë”© ì¤‘...             |                             |
T7   | ëª¨ë¸ ë¡œë”© ì™„ë£Œ              |                             |
T8   | ë½ í•´ì œ                     | ë½ íšë“ ì„±ê³µ                | ë½ íšë“ ëŒ€ê¸°
T9   | ëª¨ë¸ ë°˜í™˜                   | ë‘ ë²ˆì§¸ ì²´í¬: Not None      | ë½ íšë“ ëŒ€ê¸°
T10  |                             | ë½ í•´ì œ                     | ë½ íšë“ ì„±ê³µ
T11  |                             | ëª¨ë¸ ë°˜í™˜                   | ë‘ ë²ˆì§¸ ì²´í¬: Not None
T12  |                             |                             | ë½ í•´ì œ
T13  |                             |                             | ëª¨ë¸ ë°˜í™˜
```

**ê²€ì¦ ë°©ë²•**:

```python
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
import threading
import time
from services.llava_service import get_llava_model

results = []
lock = threading.Lock()

def request_model(thread_id):
    try:
        start_time = time.time()
        processor, model = get_llava_model()
        end_time = time.time()
        with lock:
            results.append({
                'thread_id': thread_id,
                'time': end_time - start_time,
                'model_id': id(model)  # ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ID
            })
    except Exception as e:
        print(f'[Thread {thread_id}] ì˜¤ë¥˜: {e}')

# 3ê°œ ìŠ¤ë ˆë“œ ë™ì‹œ ì‹¤í–‰
threads = []
for i in range(3):
    t = threading.Thread(target=request_model, args=(i+1,))
    threads.append(t)
    t.start()

for t in threads:
    t.join()

# ëª¨ë“  ìŠ¤ë ˆë“œê°€ ê°™ì€ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ”ì§€ í™•ì¸
model_ids = [r.get('model_id') for r in results if 'model_id' in r]
unique_model_ids = len(set(model_ids)) if model_ids else 0

print(f'ê³ ìœ  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤: {unique_model_ids}ê°œ (ì˜ˆìƒ: 1ê°œ)')
# âœ… ì„±ê³µ: 1ê°œ â†’ Thread-safe ë¡œë”© ì •ìƒ ì‘ë™
# âŒ ì‹¤íŒ¨: 3ê°œ â†’ Thread-safe ë¡œë”© ì‹¤íŒ¨
```

---

### ì˜ˆìƒ ì§ˆë¬¸ ë° ë‹µë³€

#### Q1: GPU ì„±ëŠ¥ì´ ì¢‹ë‹¤ë©´ threadë³„ë¡œ ëª¨ë¸ì„ ì—…ë¡œë“œí•´ì„œ ì“¸ ìˆ˜ë„ ìˆëŠ”ê±°ì•¼?

**ë‹µë³€**: ê¸°ìˆ ì ìœ¼ë¡œëŠ” ê°€ëŠ¥í•˜ì§€ë§Œ, í˜„ì¬ëŠ” **ì‹±ê¸€í†¤ íŒ¨í„´(ë‹¨ì¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ê³µìœ )**ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤.

**1. ê¸°ìˆ ì  ê°€ëŠ¥ì„±**

```python
# Threadë³„ ëª¨ë¸ ë¡œë”© ì˜ˆì‹œ (í˜„ì¬ëŠ” ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
_thread_models = {}  # thread_id -> (processor, model)
_thread_lock = threading.Lock()

def get_llava_model_per_thread():
    thread_id = threading.get_ident()
    
    if thread_id not in _thread_models:
        with _thread_lock:
            if thread_id not in _thread_models:
                # ê° ìŠ¤ë ˆë“œë§ˆë‹¤ ë³„ë„ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ ë¡œë“œ
                processor = LlavaProcessor.from_pretrained(...)
                model = LlavaForConditionalGeneration.from_pretrained(...)
                _thread_models[thread_id] = (processor, model)
    
    return _thread_models[thread_id]
```

**2. í˜„ì¬ ì‹±ê¸€í†¤ íŒ¨í„´ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ **

| í•­ëª© | ì‹±ê¸€í†¤ íŒ¨í„´ (í˜„ì¬) | Threadë³„ ëª¨ë¸ |
|------|-------------------|---------------|
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | ~7GB (8-bit ì–‘ìí™”) | ~7GB Ã— ìŠ¤ë ˆë“œ ìˆ˜ |
| **ë¡œë”© ì‹œê°„** | 1íšŒë§Œ ë¡œë“œ (1-2ë¶„) | ìŠ¤ë ˆë“œë§ˆë‹¤ ë¡œë“œ (1-2ë¶„ Ã— N) |
| **ë™ì‹œì„±** | ëª¨ë¸ ê³µìœ ë¡œ ì¸í•œ ì ê¸ˆ í•„ìš” | ì ê¸ˆ ì—†ì´ ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥ |
| **GPU ë©”ëª¨ë¦¬** | íš¨ìœ¨ì  (1ê°œ ëª¨ë¸) | ë¹„íš¨ìœ¨ì  (Nê°œ ëª¨ë¸) |

**3. Threadë³„ ëª¨ë¸ì´ ìœ ë¦¬í•œ ê²½ìš°**

ë‹¤ìŒ ì¡°ê±´ì„ ëª¨ë‘ ë§Œì¡±í•  ë•Œë§Œ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

- âœ… **GPU ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•œ ê²½ìš°** (ì˜ˆ: 80GB ì´ìƒ)
  - LLaVA 7B ëª¨ë¸: FP16 ê¸°ì¤€ ~14GB, 8-bit ì–‘ìí™” ê¸°ì¤€ ~7GB
  - 10ê°œ ìŠ¤ë ˆë“œ Ã— 7GB = 70GB í•„ìš”
- âœ… **ë™ì‹œ ìš”ì²­ì´ ë§¤ìš° ë§ì€ ê²½ìš°** (ì˜ˆ: ì´ˆë‹¹ 100+ ìš”ì²­)
  - ëª¨ë¸ ê³µìœ  ì‹œ ì ê¸ˆ ê²½í•©ìœ¼ë¡œ ì¸í•œ ì„±ëŠ¥ ì €í•˜ê°€ ì‹¬ê°í•  ë•Œ
- âœ… **ë¡œë”© ì‹œê°„ì´ ë¬¸ì œê°€ ì•„ë‹Œ ê²½ìš°**
  - ì›Œë°ì—… ì‹œê°„ì´ ì¶©ë¶„í•˜ê±°ë‚˜, ì„œë²„ ì‹œì‘ ì‹œ ë¯¸ë¦¬ ë¡œë“œ ê°€ëŠ¥í•  ë•Œ

**4. í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ ì‹±ê¸€í†¤ íŒ¨í„´ì´ ë” ë‚˜ì€ ì´ìœ **

1. **ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±**
   ```
   ì‹±ê¸€í†¤: 7GB (1ê°œ ëª¨ë¸)
   Threadë³„: 70GB (10ê°œ ìŠ¤ë ˆë“œ Ã— 7GB)
   â†’ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 10ë°° ì°¨ì´
   ```

2. **PyTorchì˜ ë‚´ë¶€ ìµœì í™”**
   - PyTorchëŠ” ëª¨ë¸ ì¶”ë¡  ì‹œ ë‚´ë¶€ì ìœ¼ë¡œ ë™ì‹œì„± ì²˜ë¦¬ ìµœì í™”
   - ë‹¨ì¼ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë„ ì—¬ëŸ¬ ìš”ì²­ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬ ê°€ëŠ¥
   - `model.eval()` ëª¨ë“œì—ì„œëŠ” thread-safeí•˜ê²Œ ì¶”ë¡  ê°€ëŠ¥

3. **ì‹¤ì œ ì„±ëŠ¥ ì°¨ì´**
   ```
   ì‹±ê¸€í†¤ íŒ¨í„´:
   - ìš”ì²­ ì²˜ë¦¬: ~5-10ì´ˆ/ì´ë¯¸ì§€
   - ë™ì‹œ ì²˜ë¦¬: ìˆœì°¨ ì²˜ë¦¬ (ì ê¸ˆìœ¼ë¡œ ì¸í•œ ëŒ€ê¸°)
   
   Threadë³„ ëª¨ë¸:
   - ìš”ì²­ ì²˜ë¦¬: ~5-10ì´ˆ/ì´ë¯¸ì§€ (ë™ì¼)
   - ë™ì‹œ ì²˜ë¦¬: ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
   - í•˜ì§€ë§Œ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ OOM ë°œìƒ ê°€ëŠ¥
   ```

4. **í˜„ì¬ ì‹œìŠ¤í…œ íŠ¹ì„±**
   - **ìš”ì²­ ë¹ˆë„**: ì´ˆë‹¹ ìˆ˜ì‹­ ê°œ ìˆ˜ì¤€ (ì´ˆë‹¹ 100+ ìˆ˜ì¤€ ì•„ë‹˜)
   - **GPU ë©”ëª¨ë¦¬**: 23GB (L4 GPU)
   - **ëª¨ë¸ í¬ê¸°**: 7GB (8-bit ì–‘ìí™”)
   - â†’ ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œë„ ì¶©ë¶„íˆ ì²˜ë¦¬ ê°€ëŠ¥

**5. ê²°ë¡ **

- **í˜„ì¬ ì‹œìŠ¤í…œ**: ì‹±ê¸€í†¤ íŒ¨í„´ì´ ìµœì 
  - ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
  - ì¶©ë¶„í•œ ì„±ëŠ¥ ì œê³µ
  - êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì•ˆì •ì 

- **Threadë³„ ëª¨ë¸ ê³ ë ¤ ì‹œì **:
  - GPU ë©”ëª¨ë¦¬ê°€ 80GB ì´ìƒì¼ ë•Œ
  - ì´ˆë‹¹ 100+ ìš”ì²­ì´ ì§€ì†ì ìœ¼ë¡œ ë“¤ì–´ì˜¬ ë•Œ
  - ë©”ëª¨ë¦¬ ë¹„ìš©ë³´ë‹¤ ì²˜ë¦¬ëŸ‰ì´ ë” ì¤‘ìš”í•  ë•Œ

**6. ëŒ€ì•ˆ: ë¹„ë™ê¸° ì²˜ë¦¬**

Threadë³„ ëª¨ë¸ ëŒ€ì‹ , ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ë™ì‹œì„±ì„ ë†’ì´ëŠ” ë°©ë²•ë„ ìˆìŠµë‹ˆë‹¤:

```python
# FastAPIì˜ ë¹„ë™ê¸° ì²˜ë¦¬ í™œìš©
@router.post("/api/yh/llava/stage1/validate")
async def llava_stage1_validate(body: LLaVaStage1In):
    # ë¹„ë™ê¸°ë¡œ ì²˜ë¦¬í•˜ì—¬ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ
    processor, model = get_llava_model()  # ì‹±ê¸€í†¤ ëª¨ë¸ ì‚¬ìš©
    result = await process_async(processor, model, body.image)
    return result
```

ì´ ë°©ë²•ìœ¼ë¡œ ë©”ëª¨ë¦¬ëŠ” ì ˆì•½í•˜ë©´ì„œë„ ë™ì‹œ ì²˜ë¦¬ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ë¡œê·¸ í™•ì¸**:

```bash
# ëª¨ë¸ ë¡œë”© ì‹œì‘ ë©”ì‹œì§€ê°€ 1íšŒë§Œ ë‚˜íƒ€ë‚˜ì•¼ í•¨
docker logs feedlyai-work-yh | grep -c "Loading LLaVa model"
# ì˜ˆìƒ ê²°ê³¼: 1
```

---

### 1-2. 8-bit ì–‘ìí™” ìƒì„¸ ì„¤ëª…

#### ì–‘ìí™”ë€?

ì–‘ìí™”(Quantization)ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‚®ì€ ì •ë°€ë„ë¡œ ë³€í™˜í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì´ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.

**ì •ë°€ë„ ë¹„êµ**:
- **FP32 (Float32)**: 32ë¹„íŠ¸, ì•½ 4ë°”ì´íŠ¸/íŒŒë¼ë¯¸í„°
- **FP16 (Float16)**: 16ë¹„íŠ¸, ì•½ 2ë°”ì´íŠ¸/íŒŒë¼ë¯¸í„°
- **INT8 (8-bit)**: 8ë¹„íŠ¸, ì•½ 1ë°”ì´íŠ¸/íŒŒë¼ë¯¸í„°

**LLaVA-1.5-7B ëª¨ë¸ ê¸°ì¤€**:
- FP32: ì•½ 28GB (7B Ã— 4 bytes)
- FP16: ì•½ 14GB (7B Ã— 2 bytes)
- INT8: ì•½ 7GB (7B Ã— 1 byte)

#### BitsAndBytesConfig ì„¤ì •

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,                    # 8-bit ì–‘ìí™” í™œì„±í™”
    bnb_8bit_compute_dtype=torch.float16  # ê³„ì‚° ì‹œ FP16 ì‚¬ìš©
)
```

**ì„¤ì • ì˜µì…˜ ì„¤ëª…**:

1. **`load_in_8bit=True`**:
   - ëª¨ë¸ ê°€ì¤‘ì¹˜ë¥¼ 8-bitë¡œ ì–‘ìí™”í•˜ì—¬ ë¡œë“œ
   - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì•½ 50% ê°ì†Œ

2. **`bnb_8bit_compute_dtype=torch.float16`**:
   - ì¶”ë¡  ì‹œ ê³„ì‚°ì€ FP16ìœ¼ë¡œ ìˆ˜í–‰
   - ì •í™•ë„ ì†ì‹¤ ìµœì†Œí™”
   - ì„±ëŠ¥ í–¥ìƒ (FP16 ì—°ì‚°ì´ INT8ë³´ë‹¤ ë¹ ë¦„)

#### ì–‘ìí™” ë™ì‘ ì›ë¦¬

```
[ëª¨ë¸ ë¡œë”©]
  â†“
[ê°€ì¤‘ì¹˜ ì–‘ìí™”]
  FP32 ê°€ì¤‘ì¹˜ â†’ INT8 ê°€ì¤‘ì¹˜ (ìŠ¤ì¼€ì¼ íŒ©í„° í¬í•¨)
  â†“
[ë©”ëª¨ë¦¬ì— ì €ì¥]
  INT8 í˜•ì‹ìœ¼ë¡œ ì €ì¥ (ë©”ëª¨ë¦¬ ì ˆì•½)
  â†“
[ì¶”ë¡  ì‹œ]
  INT8 ê°€ì¤‘ì¹˜ â†’ FP16ìœ¼ë¡œ ì—­ì–‘ìí™” â†’ ê³„ì‚° ìˆ˜í–‰
  â†“
[ê²°ê³¼ ë°˜í™˜]
```

**ì–‘ìí™” ê³µì‹**:
```
quantized_value = round(original_value / scale) + zero_point
```

**ì—­ì–‘ìí™” ê³µì‹**:
```
dequantized_value = (quantized_value - zero_point) Ã— scale
```

#### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

**ì‹¤ì œ ì¸¡ì • ê²°ê³¼** (LLaVA-1.5-7B, NVIDIA A100 40GB):

| ëª¨ë“œ | ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ì ˆì•½ë¥  | ì¶”ë¡  ì‹œê°„ | ì •í™•ë„ |
|------|-------------|--------|----------|--------|
| FP32 | ~28GB | - | 10ì´ˆ | 100% |
| FP16 | ~14GB | 50% | 8ì´ˆ | 99.9% |
| INT8 | ~7GB | 75% | 9ì´ˆ | 99.5% |

#### ì–‘ìí™” í™œì„±í™” ë°©ë²•

**í™˜ê²½ ë³€ìˆ˜ ì„¤ì •**:

```bash
# .env íŒŒì¼
USE_QUANTIZATION=true
```

**ì½”ë“œì—ì„œ í™•ì¸**:

```python
from config import USE_QUANTIZATION

if USE_QUANTIZATION:
    print("8-bit ì–‘ìí™” í™œì„±í™”ë¨")
else:
    print("8-bit ì–‘ìí™” ë¹„í™œì„±í™”ë¨ (FP16/FP32 ì‚¬ìš©)")
```

#### ìë™ Fallback ë©”ì»¤ë‹ˆì¦˜

ì–‘ìí™” ì‹¤íŒ¨ ì‹œ ìë™ìœ¼ë¡œ FP16ìœ¼ë¡œ ì „í™˜:

```python
if DEVICE == "cuda" and USE_QUANTIZATION:
    try:
        # 8-bit ì–‘ìí™” ì‹œë„
        quantization_config = BitsAndBytesConfig(
            load_in_8bit=True,
            bnb_8bit_compute_dtype=torch.float16
        )
        _model = LlavaForConditionalGeneration.from_pretrained(
            LLAVA_MODEL_NAME,
            quantization_config=quantization_config,
            ...
        )
        print("âœ“ Model loaded with 8-bit quantization")
    except Exception as e:
        print(f"âš  8-bit quantization failed: {e}")
        print("Falling back to standard loading...")
        # FP16ìœ¼ë¡œ fallback
        _model = LlavaForConditionalGeneration.from_pretrained(
            LLAVA_MODEL_NAME,
            torch_dtype=torch.float16,
            max_memory={0: "20GiB"},  # GPU ë©”ëª¨ë¦¬ ì œí•œ
            ...
        )
        print("âœ“ Model loaded with FP16 (quantization disabled)")
```

**Fallback ë°œìƒ ì‹œë‚˜ë¦¬ì˜¤**:
- `bitsandbytes` ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¯¸ì„¤ì¹˜
- GPUê°€ 8-bit ì–‘ìí™”ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
- ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì–‘ìí™” ì‹¤íŒ¨

#### GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

```python
# ëª¨ë¸ ë¡œë”© ì „í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì •
if DEVICE == "cuda":
    torch.cuda.reset_peak_memory_stats()
    initial_memory = torch.cuda.memory_allocated() / 1024**3  # GB
    
    # ëª¨ë¸ ë¡œë”©...
    
    loaded_memory = torch.cuda.memory_allocated() / 1024**3  # GB
    peak_memory = torch.cuda.max_memory_allocated() / 1024**3  # GB
    total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
    
    print(f"ğŸ“Š GPU Memory Usage:")
    print(f"   - Allocated: {loaded_memory:.2f} GB")
    print(f"   - Peak (during load): {peak_memory:.2f} GB")
    print(f"   - Total GPU: {total_memory:.2f} GB")
    print(f"   - Usage: {loaded_memory/total_memory*100:.1f}%")
```

**ì˜ˆìƒ ì¶œë ¥**:
```
ğŸ“Š GPU Memory Usage:
   - Allocated: 7.23 GB (8-bit ì–‘ìí™”)
   - Peak (during load): 8.45 GB
   - Total GPU: 40.00 GB
   - Usage: 18.1%
```

#### ì–‘ìí™” ì„±ëŠ¥ ì˜í–¥

**ì •í™•ë„**:
- ì¼ë°˜ì ìœ¼ë¡œ 0.1-0.5% ì •í™•ë„ ì†ì‹¤
- ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ë¬´ì‹œí•  ìˆ˜ ìˆëŠ” ìˆ˜ì¤€

**ì†ë„**:
- ì–‘ìí™”/ì—­ì–‘ìí™” ì˜¤ë²„í—¤ë“œë¡œ ì•½ 10-20% ëŠë ¤ì§ˆ ìˆ˜ ìˆìŒ
- í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ì ˆì•½ìœ¼ë¡œ ë” í° ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ì „ì²´ì ìœ¼ë¡œ ìœ ë¦¬

**ë©”ëª¨ë¦¬**:
- ì•½ 50% ë©”ëª¨ë¦¬ ì ˆì•½
- ë” ë§ì€ variantsë¥¼ ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥

---

### 2. Stage 1: ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ì¦

**íŒŒì¼**: `routers/llava_stage1.py`

```python
@router.post("/validate", response_model=LLaVaStage1Out)
def stage1_validate(body: LLaVaStage1In, db: Session = Depends(get_db)):
    """
    LLaVa Stage 1 Validation: ì´ë¯¸ì§€ì™€ ê´‘ê³ ë¬¸êµ¬ì˜ ì í•©ì„± ê²€ì¦
    
    Args:
        body: LLaVaStage1In ëª¨ë¸
            - job_variants_id: Job Variant ID
            - job_id: Job ID
            - tenant_id: Tenant ID
            - ad_copy_text: ê´‘ê³ ë¬¸êµ¬ (Optional)
            - prompt: ì»¤ìŠ¤í…€ ê²€ì¦ í”„ë¡¬í”„íŠ¸ (Optional)
    
    Returns:
        LLaVaStage1Out:
            - job_id: Job ID
            - vlm_trace_id: VLM Trace ID
            - is_valid: ì í•©ì„± ì—¬ë¶€
            - image_quality_ok: ì´ë¯¸ì§€ í’ˆì§ˆ OK ì—¬ë¶€
            - relevance_score: ê´€ë ¨ì„± ì ìˆ˜ (0.0-1.0)
            - analysis: LLaVa ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸
            - issues: ë°œê²¬ëœ ì´ìŠˆ ëª©ë¡
            - recommendations: ì¶”ì²œì‚¬í•­ ëª©ë¡
            - font_recommendation: í°íŠ¸ ì¶”ì²œ ì •ë³´
    """
    # Step 0: job_variants_id ë° job_id ê²€ì¦
    job_variants_id = uuid.UUID(body.job_variants_id)
    job_id = uuid.UUID(body.job_id)
    
    # job_variants ì¡°íšŒ
    job_variant = db.query(JobVariant).filter(
        JobVariant.job_variants_id == job_variants_id
    ).first()
    
    # job_variants ìƒíƒœ ì—…ë°ì´íŠ¸: current_step='vlm_analyze', status='running'
    db.execute(
        text("""
            UPDATE jobs_variants 
            SET status = 'running', 
                current_step = 'vlm_analyze',
                updated_at = CURRENT_TIMESTAMP
            WHERE job_variants_id = :job_variants_id
        """),
        {"job_variants_id": job_variants_id}
    )
    db.flush()
    
    # Step 1: ì´ë¯¸ì§€ ê°€ì ¸ì˜¤ê¸°
    image_asset_id = job_variant.img_asset_id
    image_asset = db.query(ImageAsset).filter(
        ImageAsset.image_asset_id == image_asset_id
    ).first()
    asset_url = image_asset.image_url
    
    # Step 2: ê´‘ê³ ë¬¸êµ¬ ì¡°íšŒ (ìš°ì„ ìˆœìœ„: body.ad_copy_text â†’ txt_ad_copy_generations â†’ job_inputs.desc_eng)
    ad_copy_text = None
    if body.ad_copy_text:
        ad_copy_text = body.ad_copy_text
    else:
        # txt_ad_copy_generationsì—ì„œ ad_copy_eng ì¡°íšŒ
        ad_copy_gen = db.execute(
            text("""
                SELECT ad_copy_eng
                FROM txt_ad_copy_generations
                WHERE job_id = :job_id
                  AND generation_stage = 'ad_copy_eng'
                  AND status = 'done'
                ORDER BY created_at DESC
                LIMIT 1
            """),
            {"job_id": job_id}
        ).first()
        
        if ad_copy_gen and ad_copy_gen.ad_copy_eng:
            ad_copy_text = ad_copy_gen.ad_copy_eng
        else:
            # job_inputsì—ì„œ desc_eng ì¡°íšŒ
            job_input = db.query(JobInput).filter(JobInput.job_id == job_id).first()
            if job_input and job_input.desc_eng:
                ad_copy_text = job_input.desc_eng
    
    # Step 3: ì´ë¯¸ì§€ ë¡œë“œ
    image_path = abs_from_url(asset_url)
    image = Image.open(image_path)
    
    # Step 4: LLaVaë¥¼ ì‚¬ìš©í•œ ê²€ì¦
    start_time = time.time()
    result = validate_image_and_text(
        image=image,
        ad_copy_text=ad_copy_text,
        validation_prompt=body.prompt
    )
    latency_ms = (time.time() - start_time) * 1000
    
    # Step 5: vlm_traces ë ˆì½”ë“œ ìƒì„±
    vlm_trace_id = uuid.uuid4()
    request_data = {
        "asset_url": asset_url,
        "ad_copy_text": ad_copy_text,
        "prompt": body.prompt
    }
    response_data = result
    
    db.execute(
        text("""
            INSERT INTO vlm_traces (
                vlm_trace_id, job_id, provider, operation_type, 
                request, response, latency_ms, created_at, updated_at
            )
            VALUES (
                :vlm_trace_id, :job_id, :provider, :operation_type,
                CAST(:request AS jsonb), CAST(:response AS jsonb), :latency_ms,
                CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
            )
        """),
        {
            "vlm_trace_id": vlm_trace_id,
            "job_id": job_id,
            "provider": "llava",
            "operation_type": "analyze",
            "request": json.dumps(request_data),
            "response": json.dumps(response_data),
            "latency_ms": latency_ms
        }
    )
    
    # Step 6: jobs_variants ìƒíƒœë¥¼ 'done'ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    db.execute(
        text("""
            UPDATE jobs_variants 
            SET status = 'done', 
                current_step = 'vlm_analyze',
                updated_at = CURRENT_TIMESTAMP
            WHERE job_variants_id = :job_variants_id
        """),
        {"job_variants_id": job_variants_id}
    )
    
    db.commit()
    
    return LLaVaStage1Out(
        job_id=body.job_id,
        vlm_trace_id=str(vlm_trace_id),
        is_valid=result.get('is_valid'),
        image_quality_ok=result.get('image_quality_ok'),
        relevance_score=result.get('relevance_score'),
        analysis=result.get('analysis', ''),
        issues=result.get('issues', []),
        recommendations=result.get('recommendations', []),
        font_recommendation=font_recommendation
    )
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- **ìƒíƒœ ê´€ë¦¬**: running â†’ doneìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ íŠ¸ë¦¬ê±° ë°œë™
- **ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ì¦**: ê´‘ê³ ë¬¸êµ¬ê°€ ì´ë¯¸ì§€ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
- **ìë™ íŠ¸ë¦¬ê±°**: done ìƒíƒœë¡œ ì—…ë°ì´íŠ¸í•˜ë©´ ë‹¤ìŒ ë‹¨ê³„ ìë™ ì‹¤í–‰
- **ì™„ì „í•œ ì¶”ì **: ëª¨ë“  í˜¸ì¶œì„ `vlm_traces`ì— ì €ì¥

---

### 3. Stage 2: ìµœì¢… í’ˆì§ˆ í‰ê°€

**íŒŒì¼**: `routers/llava_stage2.py`

```python
@router.post("/judge", response_model=JudgeOut)
def judge(body: JudgeIn, db: Session = Depends(get_db)):
    """
    LLaVa Stage 2 Validation: ìµœì¢… ê´‘ê³  ì‹œê° ê²°ê³¼ë¬¼ íŒë‹¨
    
    Args:
        body: JudgeIn ëª¨ë¸
            - job_variants_id: Job Variant ID
            - job_id: Job ID
            - tenant_id: Tenant ID
            - overlay_id: Overlay ID (Optional)
            - render_asset_url: ë Œë”ë§ëœ ì´ë¯¸ì§€ URL (Optional)
    
    Returns:
        JudgeOut:
            - job_id: Job ID
            - vlm_trace_id: VLM Trace ID
            - on_brief: brief ì¤€ìˆ˜ ì—¬ë¶€
            - occlusion: ê°€ë¦¼ ì—¬ë¶€ (Trueë©´ ê°€ë¦¼ ìˆìŒ)
            - contrast_ok: ëŒ€ë¹„ ì ì ˆì„±
            - cta_present: CTA ì¡´ì¬ ì—¬ë¶€
            - analysis: LLaVA ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸
            - issues: ë°œê²¬ëœ ì´ìŠˆ ëª©ë¡
    """
    # Step 0: job_variants_id ë° job_id ê²€ì¦
    job_variants_id = uuid.UUID(body.job_variants_id)
    job_id = uuid.UUID(body.job_id)
    
    # job_variants ì¡°íšŒ
    job_variant = db.query(JobVariant).filter(
        JobVariant.job_variants_id == job_variants_id
    ).first()
    
    # job_variant ìƒíƒœ í™•ì¸ (current_step='overlay', status='done'ì´ì–´ì•¼ í•¨)
    if job_variant.current_step != 'overlay' or job_variant.status != 'done':
        raise HTTPException(
            status_code=400,
            detail=f"Job variant ìƒíƒœê°€ judge ì‹¤í–‰ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
        )
    
    # job_variants ìƒíƒœ ì—…ë°ì´íŠ¸: current_step='vlm_judge', status='running'
    db.execute(
        text("""
            UPDATE jobs_variants 
            SET status = 'running', 
                current_step = 'vlm_judge',
                updated_at = CURRENT_TIMESTAMP
            WHERE job_variants_id = :job_variants_id
        """),
        {"job_variants_id": job_variants_id}
    )
    db.flush()
    
    # Step 1: render_asset_url ê°€ì ¸ì˜¤ê¸°
    render_asset_url = body.render_asset_url
    if not render_asset_url:
        # ìš°ì„ ìˆœìœ„ 1: job_variantì—ì„œ overlaid_img_asset_id ì¡°íšŒ
        if job_variant.overlaid_img_asset_id:
            overlaid_asset = db.query(ImageAsset).filter(
                ImageAsset.image_asset_id == job_variant.overlaid_img_asset_id
            ).first()
            if overlaid_asset:
                render_asset_url = overlaid_asset.image_url
        else:
            # ìš°ì„ ìˆœìœ„ 2: overlay_idë¡œë¶€í„° render_asset_url ì¡°íšŒ
            overlay = db.query(OverlayLayout).filter(
                OverlayLayout.overlay_id == uuid.UUID(body.overlay_id)
            ).first()
            layout = overlay.layout if isinstance(overlay.layout, dict) else json.loads(overlay.layout)
            render_asset_url = layout.get('render', {}).get('url')
    
    # Step 2: ì´ë¯¸ì§€ ë¡œë“œ
    image = Image.open(abs_from_url(render_asset_url)).convert("RGB")
    
    # Step 3: LLaVAë¥¼ ì‚¬ìš©í•œ íŒë‹¨
    start_time = time.time()
    result = judge_final_ad(image=image)
    latency_ms = (time.time() - start_time) * 1000
    
    # Step 4: vlm_tracesì— ì €ì¥
    vlm_trace_id = uuid.uuid4()
    request_data = {
        "render_asset_url": render_asset_url,
        "overlay_id": body.overlay_id
    }
    response_data = result
    
    db.execute(
        text("""
            INSERT INTO vlm_traces (
                vlm_trace_id, job_id, provider, operation_type, 
                request, response, latency_ms, created_at, updated_at
            )
            VALUES (
                :vlm_trace_id, :job_id, :provider, :operation_type,
                CAST(:request AS jsonb), CAST(:response AS jsonb), :latency_ms,
                CURRENT_TIMESTAMP, CURRENT_TIMESTAMP
            )
        """),
        {
            "vlm_trace_id": vlm_trace_id,
            "job_id": job_id,
            "provider": "llava",
            "operation_type": "judge",
            "request": json.dumps(request_data),
            "response": json.dumps(response_data),
            "latency_ms": latency_ms
        }
    )
    
    # Step 5: jobs_variants ìƒíƒœë¥¼ 'done'ìœ¼ë¡œ ì—…ë°ì´íŠ¸
    db.execute(
        text("""
            UPDATE jobs_variants 
            SET status = 'done', 
                current_step = 'vlm_judge',
                updated_at = CURRENT_TIMESTAMP
            WHERE job_variants_id = :job_variants_id
        """),
        {"job_variants_id": job_variants_id}
    )
    
    db.commit()
    
    return JudgeOut(
        job_id=body.job_id,
        vlm_trace_id=str(vlm_trace_id),
        on_brief=result.get("on_brief", False),
        occlusion=result.get("occlusion", False),
        contrast_ok=result.get("contrast_ok", False),
        cta_present=result.get("cta_present", False),
        analysis=result.get("analysis", ""),
        issues=result.get("issues", [])
    )
```

**í•µì‹¬ í¬ì¸íŠ¸**:
- **ì˜¤ë²„ë ˆì´ í’ˆì§ˆ í‰ê°€**: í…ìŠ¤íŠ¸ê°€ ì˜ ë³´ì´ëŠ”ì§€, ê°€ë…ì„±ì´ ì¢‹ì€ì§€ í‰ê°€
- **ìë™ íŠ¸ë¦¬ê±°**: ë‹¤ìŒ ë‹¨ê³„(ocr_eval)ë¡œ ìë™ ì§„í–‰
- **ì™„ì „í•œ ì¶”ì **: ëª¨ë“  í˜¸ì¶œì„ `vlm_traces`ì— ì €ì¥

---

### 4. ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ê²€ì¦ ë¡œì§

**íŒŒì¼**: `services/llava_service.py`

```python
def validate_image_and_text(
    image: Image.Image,
    ad_copy_text: Optional[str] = None,
    validation_prompt: Optional[str] = None
) -> Dict[str, Any]:
    """
    ì´ë¯¸ì§€ì™€ ê´‘ê³ ë¬¸êµ¬ì˜ ì í•©ì„± ê²€ì¦
    
    Args:
        image: PIL Image ê°ì²´
        ad_copy_text: ê´‘ê³ ë¬¸êµ¬ í…ìŠ¤íŠ¸
        validation_prompt: ì»¤ìŠ¤í…€ ê²€ì¦ í”„ë¡¬í”„íŠ¸
    
    Returns:
        {
            "is_valid": bool,
            "image_quality_ok": bool,
            "relevance_score": float,
            "analysis": str,
            "issues": List[str],
            "recommendations": List[str],
            "font_recommendation": Dict
        }
    """
    processor, model = get_llava_model()
    
    # ê¸°ë³¸ ê²€ì¦ í”„ë¡¬í”„íŠ¸
    if validation_prompt is None:
        if ad_copy_text:
            validation_prompt = f"""Analyze this image and the following ad copy text:
"{ad_copy_text}"

Please evaluate:
1. Does the image match the ad copy text logically?
2. Is the image quality good (clear, well-lit, appropriate)?
3. What is the relevance score (0.0-1.0) between the image and text?
4. Are there any issues or concerns?
5. What recommendations do you have?
6. What font style would be appropriate for text overlay?

Respond in JSON format:
{{
    "is_valid": true/false,
    "image_quality_ok": true/false,
    "relevance_score": 0.0-1.0,
    "analysis": "detailed analysis text",
    "issues": ["issue1", "issue2"],
    "recommendations": ["recommendation1", "recommendation2"],
    "font_recommendation": {{
        "style": "bold/serif/sans-serif",
        "size": "large/medium/small",
        "color": "light/dark"
    }}
}}"""
        else:
            validation_prompt = """Analyze this image and evaluate its quality and suitability for advertising.
Respond in JSON format with is_valid, image_quality_ok, relevance_score, analysis, issues, and recommendations."""
    
    # LLaVA í”„ë¡¬í”„íŠ¸ í˜•ì‹: USER: <image>\n{prompt}\nASSISTANT:
    formatted_prompt = f"USER: <image>\n{validation_prompt}\nASSISTANT:"
    
    # ì´ë¯¸ì§€ì™€ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
    inputs = processor(images=[image], text=formatted_prompt, return_tensors="pt")
    
    # GPUë¡œ ì´ë™
    if DEVICE == "cuda":
        inputs = {k: v.to(DEVICE) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
    
    # ì¶”ë¡ 
    with torch.no_grad():
        generate_ids = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.1,
            do_sample=False
        )
    
    # ì‘ë‹µ ë””ì½”ë”©
    response = processor.batch_decode(
        generate_ids, 
        skip_special_tokens=True, 
        clean_up_tokenization_spaces=False
    )[0]
    
    # JSON íŒŒì‹± ë° ê²°ê³¼ ë°˜í™˜
    # (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ íŒŒì‹± ë¡œì§ì´ í•„ìš”)
    result = parse_llava_response(response)
    
    return result
```

---

## ğŸ”„ íŒŒì´í”„ë¼ì¸ í†µí•©

### Stage 1 íë¦„
```
[img_gen ì™„ë£Œ]
  â†“
[vlm_analyze íŠ¸ë¦¬ê±°]
  â†“
[LLaVA Stage 1 ê²€ì¦]
  â†“
[ê²°ê³¼ ì €ì¥ (vlm_traces)]
  â†“
[jobs_variants ìƒíƒœ ì—…ë°ì´íŠ¸: done]
  â†“
[yolo_detect ìë™ íŠ¸ë¦¬ê±°]
```

### Stage 2 íë¦„
```
[overlay ì™„ë£Œ]
  â†“
[vlm_judge íŠ¸ë¦¬ê±°]
  â†“
[LLaVA Stage 2 íŒë‹¨]
  â†“
[ê²°ê³¼ ì €ì¥ (vlm_traces)]
  â†“
[jobs_variants ìƒíƒœ ì—…ë°ì´íŠ¸: done]
  â†“
[ocr_eval ìë™ íŠ¸ë¦¬ê±°]
```

---

## ğŸ“Š ì„±ëŠ¥ ë° í†µê³„

### ëª¨ë¸ ë¡œë”©
- **ë¡œë”© ì‹œê°„**: ì•½ 1-2ë¶„ (ìµœì´ˆ 1íšŒ)
- **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: 
  - FP16: ì•½ 10-15GB
  - 8-bit ì–‘ìí™”: ì•½ 5-8GB (ì•½ 50% ê°ì†Œ)

### ì¶”ë¡  ì„±ëŠ¥
- **ì¶”ë¡  ì‹œê°„**: ì•½ 5-10ì´ˆ (ì´ë¯¸ì§€ë‹¹)
- **ì²˜ë¦¬ëŸ‰**: GPU í™˜ê²½ì—ì„œ ì´ˆë‹¹ ì•½ 0.1-0.2 ì´ë¯¸ì§€

### ì •í™•ë„
- **ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ì¼ì¹˜ë„**: ë†’ì€ ì •í™•ë„ë¡œ ê²€ì¦
- **í’ˆì§ˆ í‰ê°€**: ì¼ê´€ëœ í‰ê°€ ê²°ê³¼ ì œê³µ

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: ëª¨ë¸ì´ ì—¬ëŸ¬ ë²ˆ ë¡œë“œë¨

**ì¦ìƒ**: ë¡œê·¸ì— "Loading LLaVa model" ë©”ì‹œì§€ê°€ ì—¬ëŸ¬ ë²ˆ ë‚˜íƒ€ë‚¨

**ì›ì¸**: Thread-safe ë¡œë”©ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŒ

**ê°€ëŠ¥í•œ ì›ì¸**:
1. `_model_lock`ì´ ì œëŒ€ë¡œ ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
2. Double-checked locking íŒ¨í„´ì´ ì˜ëª» êµ¬í˜„ë¨
3. ì „ì—­ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ê³µìœ ë˜ì§€ ì•ŠìŒ
4. ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¬ì‹œì‘ë˜ì§€ ì•Šì•„ì„œ ì´ì „ ì½”ë“œê°€ ì‹¤í–‰ ì¤‘

**í•´ê²° ë°©ë²•**:
1. `_model_lock`ì´ ì œëŒ€ë¡œ ì‚¬ìš©ë˜ê³  ìˆëŠ”ì§€ í™•ì¸
   ```python
   # services/llava_service.py í™•ì¸
   _model_lock = threading.Lock()  # ì „ì—­ ë³€ìˆ˜ë¡œ ì„ ì–¸ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
   ```
2. Double-checked locking íŒ¨í„´ í™•ì¸
   ```python
   # ì²« ë²ˆì§¸ ì²´í¬ì™€ ë‘ ë²ˆì§¸ ì²´í¬ê°€ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
   if _model is None:  # ì²« ë²ˆì§¸ ì²´í¬
       with _model_lock:
           if _model is None:  # ë‘ ë²ˆì§¸ ì²´í¬ (í•„ìˆ˜!)
               # ëª¨ë¸ ë¡œë”©
   ```
3. ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘
   ```bash
   docker-compose restart yh
   ```
4. ë¡œê·¸ í™•ì¸
   ```bash
   docker logs feedlyai-work-yh | grep -c "Loading LLaVa model"
   # ì˜ˆìƒ: 1íšŒë§Œ ë‚˜íƒ€ë‚˜ì•¼ í•¨
   ```

**ë””ë²„ê¹… ë°©ë²•**:
```python
# Thread-safe ë¡œë”© í…ŒìŠ¤íŠ¸
import threading
import time
from services.llava_service import get_llava_model

def test_thread_safe_loading():
    results = []
    lock = threading.Lock()
    
    def request_model(thread_id):
        try:
            start_time = time.time()
            processor, model = get_llava_model()
            end_time = time.time()
            with lock:
                results.append({
                    'thread_id': thread_id,
                    'time': end_time - start_time,
                    'model_id': id(model)
                })
        except Exception as e:
            print(f'[Thread {thread_id}] ì˜¤ë¥˜: {e}')
    
    # 5ê°œ ìŠ¤ë ˆë“œ ë™ì‹œ ì‹¤í–‰
    threads = []
    for i in range(5):
        t = threading.Thread(target=request_model, args=(i+1,))
        threads.append(t)
        t.start()
    
    for t in threads:
        t.join()
    
    # ê²°ê³¼ ë¶„ì„
    model_ids = [r.get('model_id') for r in results if 'model_id' in r]
    unique_model_ids = len(set(model_ids)) if model_ids else 0
    
    print(f'ì´ ìŠ¤ë ˆë“œ ìˆ˜: {len(results)}')
    print(f'ê³ ìœ  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤: {unique_model_ids}ê°œ (ì˜ˆìƒ: 1ê°œ)')
    
    if unique_model_ids == 1:
        print('âœ… Thread-safe ë¡œë”© ì •ìƒ ì‘ë™!')
    else:
        print(f'âŒ Thread-safe ë¡œë”© ì‹¤íŒ¨! {unique_model_ids}ê°œì˜ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë¨')
    
    # ë¡œë”© ì‹œê°„ ë¶„ì„
    loading_times = [r.get('time', 0) for r in results]
    print(f'ë¡œë”© ì‹œê°„: {loading_times}')
    if max(loading_times) > 10:  # 10ì´ˆ ì´ìƒì´ë©´ ì‹¤ì œ ë¡œë”©ì´ ë°œìƒí•œ ê²ƒ
        print('âš ï¸ ì¼ë¶€ ìŠ¤ë ˆë“œì—ì„œ ì‹¤ì œ ëª¨ë¸ ë¡œë”©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤')
```

**ì˜ˆìƒ ê²°ê³¼**:
```
ì´ ìŠ¤ë ˆë“œ ìˆ˜: 5
ê³ ìœ  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤: 1ê°œ (ì˜ˆìƒ: 1ê°œ)
âœ… Thread-safe ë¡œë”© ì •ìƒ ì‘ë™!
ë¡œë”© ì‹œê°„: [0.001, 0.001, 0.001, 0.001, 0.001]  # ëª¨ë‘ ë¹ ë¥´ê²Œ ë°˜í™˜
```

---

### ë¬¸ì œ 2: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±

**ì¦ìƒ**: CUDA out of memory ì˜¤ë¥˜

**ì›ì¸ ë¶„ì„**:
1. ëª¨ë¸ì´ FP16/FP32ë¡œ ë¡œë“œë˜ì–´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ í¼
2. ì—¬ëŸ¬ variantsê°€ ë™ì‹œì— ì‹¤í–‰ë˜ì–´ ë©”ëª¨ë¦¬ ë¶€ì¡±
3. ì–‘ìí™”ê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŒ

**í•´ê²° ë°©ë²•**:

1. **8-bit ì–‘ìí™” í™œì„±í™”** (ê°€ì¥ íš¨ê³¼ì ):
   ```bash
   # .env íŒŒì¼
   USE_QUANTIZATION=true
   ```
   
   **ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼**:
   - FP16: ~14GB â†’ INT8: ~7GB (ì•½ 50% ì ˆì•½)
   - ë” ë§ì€ variantsë¥¼ ë™ì‹œì— ì²˜ë¦¬ ê°€ëŠ¥

2. **ëª¨ë¸ í¬ê¸° í™•ì¸ ë° ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©**:
   ```python
   # config.py
   LLAVA_MODEL_NAME = "llava-hf/llava-1.5-7b-hf"  # 7B ëª¨ë¸ (ê¸°ë³¸)
   # ë˜ëŠ”
   LLAVA_MODEL_NAME = "llava-hf/llava-1.5-13b-hf"  # 13B ëª¨ë¸ (ë” ì •í™•í•˜ì§€ë§Œ ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©)
   ```

3. **GPU ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì •**:
   ```python
   _model = LlavaForConditionalGeneration.from_pretrained(
       LLAVA_MODEL_NAME,
       torch_dtype=torch.float16,
       device_map="auto",
       max_memory={0: "20GiB"}  # GPU ë©”ëª¨ë¦¬ ì œí•œ
   )
   ```

4. **ë©”ëª¨ë¦¬ ì •ë¦¬**:
   ```python
   # ì¶”ë¡  í›„ ë©”ëª¨ë¦¬ ì •ë¦¬
   if DEVICE == "cuda":
       torch.cuda.empty_cache()
   ```

**í™•ì¸ ë°©ë²•**:
```bash
# GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
nvidia-smi

# ë˜ëŠ” Pythonì—ì„œ í™•ì¸
python3 -c "
import torch
if torch.cuda.is_available():
    print(f'GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')
    print(f'GPU ë©”ëª¨ë¦¬ ìµœëŒ€ ì‚¬ìš©ëŸ‰: {torch.cuda.max_memory_allocated() / 1024**3:.2f} GB')
    print(f'GPU ì´ ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB')
"
```

**ì–‘ìí™” í™œì„±í™” í™•ì¸**:
```bash
# ë¡œê·¸ì—ì„œ ì–‘ìí™” ì„¤ì • í™•ì¸
docker logs feedlyai-work-yh | grep "Quantization setting"
# ì˜ˆìƒ ì¶œë ¥: "Quantization setting: Enabled (8-bit)"
```

**ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ**:
```bash
# ì–‘ìí™” í™œì„±í™” ì „
nvidia-smi
# ì˜ˆìƒ: ~14GB ì‚¬ìš©

# ì–‘ìí™” í™œì„±í™” í›„
nvidia-smi
# ì˜ˆìƒ: ~7GB ì‚¬ìš© (ì•½ 50% ê°ì†Œ)
```

---

### ë¬¸ì œ 3: ëª¨ë¸ ë¡œë”© ì‹œê°„ì´ ë„ˆë¬´ ê¹€

**ì¦ìƒ**: ì²« ìš”ì²­ ì‹œ ì‘ë‹µ ì‹œê°„ì´ ë§¤ìš° ê¹€ (1-2ë¶„)

**ì›ì¸**: ëª¨ë¸ì„ ì²˜ìŒ ë¡œë“œí•  ë•Œ ì‹œê°„ì´ ì†Œìš”ë¨

**í•´ê²° ë°©ë²•**:
- ì •ìƒì ì¸ ë™ì‘ì…ë‹ˆë‹¤
- ëª¨ë¸ì€ í•œ ë²ˆë§Œ ë¡œë“œë˜ë¯€ë¡œ ì´í›„ ìš”ì²­ì€ ë¹ ë¦…ë‹ˆë‹¤
- ì›Œë°ì—… ìš”ì²­ì„ ë¯¸ë¦¬ ë³´ë‚´ëŠ” ê²ƒì„ ê³ ë ¤í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

---

### ë¬¸ì œ 4: ì¶”ë¡  ê²°ê³¼ê°€ ì¼ê´€ë˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: ê°™ì€ ì´ë¯¸ì§€ì— ëŒ€í•´ ë‹¤ë¥¸ ê²°ê³¼ê°€ ë‚˜ì˜´

**ì›ì¸**: `temperature` ì„¤ì •ì´ ë„ˆë¬´ ë†’ìŒ

**í•´ê²° ë°©ë²•**:
```python
# temperatureë¥¼ ë‚®ê²Œ ì„¤ì • (ê¸°ë³¸ê°’: 0.1)
generate_ids = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.1,  # ë‚®ì€ ê°’ìœ¼ë¡œ ì„¤ì •
    do_sample=False   # ìƒ˜í”Œë§ ë¹„í™œì„±í™”
)
```

---

### ë¬¸ì œ 5: JSON íŒŒì‹± ì˜¤ë¥˜

**ì¦ìƒ**: LLaVA ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ

**ì›ì¸**: LLaVAê°€ JSON í˜•ì‹ì´ ì•„ë‹Œ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜

**í•´ê²° ë°©ë²•**:
1. í”„ë¡¬í”„íŠ¸ì— JSON í˜•ì‹ ëª…ì‹œ
   ```python
   validation_prompt = f"""...
   Respond in JSON format:
   {{
       "is_valid": true/false,
       "relevance_score": 0.0-1.0,
       ...
   }}"""
   ```

2. íŒŒì‹± ì‹¤íŒ¨ ì‹œ fallback ë¡œì§ êµ¬í˜„
   ```python
   import json
   import re
   
   def parse_llava_response(response: str) -> Dict[str, Any]:
       try:
           # JSON ë¸”ë¡ ì¶”ì¶œ
           json_match = re.search(r'\{[^{}]*\}', response, re.DOTALL)
           if json_match:
               return json.loads(json_match.group())
       except json.JSONDecodeError:
           pass
       
       # Fallback: ê¸°ë³¸ê°’ ë°˜í™˜
       return {
           "is_valid": False,
           "relevance_score": 0.0,
           "analysis": response,
           "issues": ["JSON íŒŒì‹± ì‹¤íŒ¨"],
           "recommendations": []
       }
   ```

3. ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ JSON ì¶”ì¶œ
   ```python
   import re
   
   # JSON ë¸”ë¡ ì°¾ê¸°
   json_pattern = r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}'
   matches = re.findall(json_pattern, response, re.DOTALL)
   if matches:
       # ê°€ì¥ ê¸´ JSON ë¸”ë¡ ì‚¬ìš©
       json_str = max(matches, key=len)
       result = json.loads(json_str)
   ```

---

### ë¬¸ì œ 6: ì–‘ìí™” í™œì„±í™”í–ˆëŠ”ë° ë©”ëª¨ë¦¬ê°€ ì¤„ì–´ë“¤ì§€ ì•ŠìŒ

**ì¦ìƒ**: `USE_QUANTIZATION=true`ë¡œ ì„¤ì •í–ˆì§€ë§Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë™ì¼

**ì›ì¸**:
1. ì• í”Œë¦¬ì¼€ì´ì…˜ì´ ì¬ì‹œì‘ë˜ì§€ ì•ŠìŒ
2. ì–‘ìí™”ê°€ ì‹¤íŒ¨í•˜ê³  fallbackì´ ë°œìƒí–ˆì§€ë§Œ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì§€ ì•ŠìŒ
3. `bitsandbytes` ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ

**í•´ê²° ë°©ë²•**:
1. ì• í”Œë¦¬ì¼€ì´ì…˜ ì¬ì‹œì‘
   ```bash
   docker-compose restart yh
   ```

2. ë¡œê·¸ í™•ì¸
   ```bash
   docker logs feedlyai-work-yh | grep -E "quantization|Quantization"
   # ì˜ˆìƒ ì¶œë ¥:
   # "Quantization setting: Enabled (8-bit)"
   # "âœ“ Model loaded with 8-bit quantization for memory efficiency"
   ```

3. `bitsandbytes` ì„¤ì¹˜ í™•ì¸
   ```bash
   docker exec feedlyai-work-yh pip list | grep bitsandbytes
   # ì˜ˆìƒ ì¶œë ¥: bitsandbytes 0.41.0 (ë˜ëŠ” ìœ ì‚¬í•œ ë²„ì „)
   ```

4. ì–‘ìí™” ì‹¤íŒ¨ ì‹œ fallback í™•ì¸
   ```bash
   docker logs feedlyai-work-yh | grep -E "quantization failed|Falling back"
   # ì–‘ìí™” ì‹¤íŒ¨ ì‹œ:
   # "âš  8-bit quantization failed: ..."
   # "Falling back to standard loading..."
   ```

---

### ë¬¸ì œ 7: Thread-safe ë¡œë”©ì´ ì‘ë™í•˜ì§€ ì•ŠìŒ

**ì¦ìƒ**: ì—¬ëŸ¬ ìŠ¤ë ˆë“œì—ì„œ ëª¨ë¸ì´ ì¤‘ë³µ ë¡œë“œë¨

**ì›ì¸**:
1. ì „ì—­ ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ê³µìœ ë˜ì§€ ì•ŠìŒ
2. `_model_lock`ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•ŠìŒ
3. ëª¨ë“ˆì´ ì—¬ëŸ¬ ë²ˆ importë¨

**í•´ê²° ë°©ë²•**:
1. ì „ì—­ ë³€ìˆ˜ í™•ì¸
   ```python
   # services/llava_service.py ìƒë‹¨
   _processor: Optional[LlavaProcessor] = None
   _model: Optional[LlavaForConditionalGeneration] = None
   _model_lock = threading.Lock()  # ëª¨ë“ˆ ë ˆë²¨ì—ì„œ ì„ ì–¸
   ```

2. ëª¨ë“ˆ import í™•ì¸
   ```python
   # ê°™ì€ ëª¨ë“ˆì—ì„œë§Œ import
   from services.llava_service import get_llava_model
   # âŒ ì˜ëª»ëœ ë°©ë²•: ë‹¤ë¥¸ ê²½ë¡œë¡œ importí•˜ë©´ ë‹¤ë¥¸ ëª¨ë“ˆ ì¸ìŠ¤í„´ìŠ¤
   ```

3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰
   ```bash
   docker exec feedlyai-work-yh python3 -c "
   import threading
   from services.llava_service import get_llava_model
   
   results = []
   def test(thread_id):
       p, m = get_llava_model()
       results.append(id(m))
   
   threads = [threading.Thread(target=test, args=(i,)) for i in range(3)]
   for t in threads: t.start()
   for t in threads: t.join()
   
   print(f'ê³ ìœ  ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤: {len(set(results))}ê°œ (ì˜ˆìƒ: 1ê°œ)')
   "
   ```

---

## ğŸ¯ ì£¼ìš” í¬ì¸íŠ¸

### ì¥ì 
- âœ… ë©€í‹°ëª¨ë‹¬ AI í™œìš© (ì´ë¯¸ì§€ + í…ìŠ¤íŠ¸)
- âœ… Thread-safe ëª¨ë¸ ë¡œë”©ìœ¼ë¡œ ë©”ëª¨ë¦¬ íš¨ìœ¨ì 
- âœ… 8-bit ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
- âœ… 2ë‹¨ê³„ ê²€ì¦ ì‹œìŠ¤í…œìœ¼ë¡œ í’ˆì§ˆ ë³´ì¥
- âœ… ì™„ì „í•œ ì¶”ì  ì‹œìŠ¤í…œ (vlm_traces)

### í™œìš© ì‚¬ë¡€
- ê´‘ê³  ì´ë¯¸ì§€ì™€ ê´‘ê³ ë¬¸êµ¬ì˜ ì í•©ì„± ê²€ì¦
- ì˜¤ë²„ë ˆì´ëœ ì´ë¯¸ì§€ì˜ ìµœì¢… í’ˆì§ˆ í‰ê°€
- í°íŠ¸ ì¶”ì²œ ë° ë””ìì¸ ê°€ì´ë“œ ì œê³µ

---

## ğŸ“š ê´€ë ¨ ë¬¸ì„œ

- `test/README_THREAD_SAFE_TEST.md`: Thread-safe ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
- `DOCS_YH_PART_IMPLEMENTATION.md`: YH íŒŒíŠ¸ êµ¬í˜„ ê°€ì´ë“œ
- `ANALYSIS_LLAVA_FONT_RECOMMENDATION.md`: LLaVA í°íŠ¸ ì¶”ì²œ ë¶„ì„

---

**ì‘ì„±ì¼**: 2025-12-02  
**ì‘ì„±ì**: LEEYH205  
**ë²„ì „**: 1.0.0

